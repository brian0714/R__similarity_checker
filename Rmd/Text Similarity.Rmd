---
title: "Text Similarity"
author: "Brian"
date: "2024-10-06"
output: html_document
---

## Install packages

```{r}
# Load data
install.packages("dplyr")
install.packages("readr")
install.packages("writexl")

# NLP
install.packages("tm")
install.packages("textTinyR")

# Similarity
install.packages("stringdist")
install.packages("proxy")
install.packages("gmp")
```

## Load Data

```{r}
# Load necessary libraries
library(dplyr)
library(rlang)
library(readr)

# csv_reader function
csv_reader <- function(csv_file_path, filter_conditions = list(), num_rows = NULL) {

  # Read CSV data
  data <- read_csv(csv_file_path)

  # Transform user_id as integer and sorting in ascending order
  data <- data %>%
    mutate(user_id = as.integer(user_id),
           use_ai = as.numeric(use_ai),
           use_all = as.numeric(use_all),
           use_revise = as.numeric(use_revise),
           use_refine = as.numeric(use_refine),
           use_reject = as.numeric(use_reject)
           ) %>%

    arrange(user_id)

  # Applying multiple filter conditions (if any)
  if (length(filter_conditions) > 0) {
    # Loop through each condition and apply using rlang's eval and parse
    for (condition in filter_conditions) {
      data <- data %>% filter(!!parse_expr(condition))
    }
  }

  # Return head "n" of rows as filtered data if specified num_rows
  if (!is.null(num_rows)) {
    data <- head(data, num_rows)
  }

  return(data)
}

# Test the function
# Set the input file path
file_path <- 'data/text_data/extracted_behavior_pattern_data.csv'

# Define multiple filter conditions as strings
filter_conditions <- list(
  "use_ai == 0",
  "task_type == 'PRACTICAL'"
)

# Call the function
df <- csv_reader(file_path, filter_conditions = filter_conditions)

# Show the number of rows
print(nrow(df))
```

## NLP tokenize and vectorize

```{r}
# Load required libraries
library(tm)
library(textTinyR)

# Bag-of-Words Vectorizer
bow_vectorize <- function(tokens, all_tokens) {
  sapply(all_tokens, function(token) sum(tokens == token))
}

# One-hot Encoding Vectorizer
one_hot_vectorize <- function(tokens, all_tokens) {
  sapply(all_tokens, function(token) ifelse(token %in% tokens, 1, 0))
}

# TF-IDF Vectorizer
tfidf_vectorize <- function(texts) {
  dtm <- DocumentTermMatrix(Corpus(VectorSource(texts)), control = list(weighting = weightTfIdf))
  tfidf_matrix <- as.matrix(dtm)
  return(tfidf_matrix)
}

# Tokenization methods
tokenize <- function(text, method = "word") {
  text <- tolower(text)
  if (method == "word") {
    tokens <- unlist(strsplit(text, "\\W+"))
  } else if (method == "character") {
    tokens <- unlist(strsplit(text, ""))
  } else if (method == "bigram") {
    tokens <- unlist(lapply(1:(nchar(text) - 1), function(i) substr(text, i, i+1)))
  } else if (method == "trigram") {
    tokens <- unlist(lapply(1:(nchar(text) - 2), function(i) substr(text, i, i+2)))
  } else {
    stop("Unknown tokenization method")
  }
  return(tokens)
}

# Remove stopwords
remove_stopwords <- function(tokens) {
  stopwords_list <- stopwords("en")
  tokens <- tokens[!tokens %in% stopwords_list]
  return(tokens)
}

# Text Vectorizer
text_vectorizer <- function(text1, text2, tokenize_method = "word", vectorize_method = "bow") {

  # Tokenize texts
  tokens1 <- remove_stopwords(tokenize(text1, tokenize_method))
  tokens2 <- remove_stopwords(tokenize(text2, tokenize_method))

  # Merge all tokens
  all_tokens <- unique(c(tokens1, tokens2))

  # Different vectorize methods
  if (vectorize_method == "bow") {
    vec1 <- bow_vectorize(tokens1, all_tokens)
    vec2 <- bow_vectorize(tokens2, all_tokens)
  } else if (vectorize_method == "one_hot") {
    vec1 <- one_hot_vectorize(tokens1, all_tokens)
    vec2 <- one_hot_vectorize(tokens2, all_tokens)
  } else if (vectorize_method == "tfidf") {
    tfidf_matrix <- tfidf_vectorize(c(text1, text2))
    vec1 <- tfidf_matrix[1, ]
    vec2 <- tfidf_matrix[2, ]
  } else {
    stop("Unknown vectorize method")
  }

  return(list(vec1, vec2))
}

# Example usage
text1 <- "The cat sits on the mat."
text2 <- "The dog sits on the mat."

result <- text_vectorizer(text1, text2, tokenize_method = "word", vectorize_method = "bow")
print(result)
```

## Similarity Methods

```{r}
# Load required libraries
library(tm)
library(stringdist)
library(proxy)
library(textTinyR)
library(gmp)


# Euclidean Similarity
euclidean_similarity <- function(text1, text2, tokenize_method = "word", vectorize_method = "bow") {
  # Use the custom text_vectorizer function to generate vectors
  vectors <- text_vectorizer(text1, text2, tokenize_method, vectorize_method)
  vec1 <- vectors[[1]]
  vec2 <- vectors[[2]]

  # Ensure the vectors have the same length
  if (length(vec1) != length(vec2)) {
    stop("Vectors must have the same length")
  }

  # Calculate the Euclidean distance
  distance <- sqrt(sum((vec1 - vec2) ^ 2))

  # Calculate similarity based on the distance
  similarity <- 1 / (1 + distance)
  return(similarity)
}

# Jaccard Similarity
jaccard_similarity <- function(text1, text2, ngram = NULL) {
  words1 <- unlist(strsplit(text1, "\\s+"))
  words2 <- unlist(strsplit(text2, "\\s+"))

  if (!is.null(ngram)) {
    set1 <- unique(textTinyR::ngram_as_strings(words1, ngram))
    set2 <- unique(textTinyR::ngram_as_strings(words2, ngram))
  } else {
    set1 <- unique(words1)
    set2 <- unique(words2)
  }

  intersection <- length(intersect(set1, set2))
  union <- length(union(set1, set2))

  return(intersection / union)
}

# Cosine Similarity
cosine_similarity <- function(text1, text2) {
  freq1 <- table(unlist(strsplit(text1, "\\s+")))
  freq2 <- table(unlist(strsplit(text2, "\\s+")))

  common <- intersect(names(freq1), names(freq2))
  if (length(common) == 0) return(0)

  vec1 <- freq1[common]
  vec2 <- freq2[common]

  similarity <- sum(vec1 * vec2) / (sqrt(sum(vec1 ^ 2)) * sqrt(sum(vec2 ^ 2)))
  return(round(similarity, 2))
}

# Levenshtein Distance
levenshtein_distance <- function(text1, text2) {
  return(stringdist::stringdist(text1, text2, method = "lv"))
}

normalized_levenshtein_distance <- function(text1, text2) {
  max_len <- max(nchar(text1), nchar(text2))
  return(levenshtein_distance(text1, text2) / max_len)
}

# Hamming Distance
hamming_distance <- function(text1, text2) {
  if (nchar(text1) != nchar(text2)) {
    stop("Strings must be of equal length")
  }

  return(sum(unlist(strsplit(text1, "")) != unlist(strsplit(text2, ""))))
}

normalized_hamming_distance <- function(text1, text2) {
  return(hamming_distance(text1, text2) / nchar(text1))
}

# Overlap Coefficient
overlap_coefficient <- function(text1, text2) {
  set1 <- unique(unlist(strsplit(text1, "\\s+")))
  set2 <- unique(unlist(strsplit(text2, "\\s+")))

  intersection <- length(intersect(set1, set2))
  return(intersection / min(length(set1), length(set2)))
}

# Winnowing Algorithm (simplified for R)
winnow_tokenize <- function(text) {
  return(tolower(unlist(strsplit(text, "\\W+"))))
}

hash_tokens <- function(tokens) {
  # Filter out empty strings or invalid tokens
  tokens <- tokens[tokens != "" & !is.na(tokens)]

  # Perform hash conversion
  hashes <- sapply(tokens, function(token) {
    hash_value <- tryCatch({
      # Try calculating the hash value
      digest_value <- digest::digest(token, algo = "sha1")
      substr(digest_value, 1, 8)  # Keep the first 8 characters of the hash value
    }, error = function(e) {
      # If an error occurs, print the invalid token and return NA
      cat("Invalid token:", token, "\n")
      NA
    })

    # Convert the hash to a big integer using gmp
    hash_numeric <- as.bigz(paste0("0x", hash_value))  # Convert hex string to big integer
    if (is.na(hash_numeric)) {
      cat("Failed to convert hash value (non-numeric):", hash_value, " for token: ", token, "\n")
    }
    as.numeric(hash_numeric)  # Convert big integer to numeric if possible
  })

  # Check if there are any NAs and remove them
  invalid_tokens <- tokens[is.na(hashes)]

  if (length(invalid_tokens) > 0) {
    cat("Invalid tokens:", paste(invalid_tokens, collapse = ", "), "\n")
  }

  # Return valid hash values
  hashes <- hashes[!is.na(hashes)]
  return(hashes)
}

k_grams <- function(hashes, k) {
  return(embed(hashes, k)[, k:1, drop = FALSE])
}

fingerprints <- function(k_grams, w) {
  window_min <- apply(embed(k_grams, w), 1, min)
  return(unique(window_min))
}

winnowing <- function(doc1, doc2, k = 1, w = 2) {
  # tokens1 <- winnow_tokenize(doc1)
  # tokens2 <- winnow_tokenize(doc2)
  tokens1 <- tokenize(doc1)
  tokens2 <- tokenize(doc2)

  hashes1 <- hash_tokens(tokens1)
  hashes2 <- hash_tokens(tokens2)

  k_grams1 <- k_grams(hashes1, k)
  k_grams2 <- k_grams(hashes2, k)

  fingerprints1 <- fingerprints(k_grams1, w)
  fingerprints2 <- fingerprints(k_grams2, w)

  if (length(fingerprints1) == 0 || length(fingerprints2) == 0) return(0)

  matches <- length(intersect(fingerprints1, fingerprints2))
  return(matches / min(length(fingerprints1), length(fingerprints2)))
}

# Usage examples
## Case 1 - Similar texts
# text1 <- "I like to read."
# text2 <- "I love to read."

# Case 2 - Similar meanings but different texts
text1 = "During weekends, I like to read books."
text2 = "I love to read books on Saturday and Sunday."

## Case 3 - Different texts
# text1 = "The research is about similarity calculation."
# text2 = "Multiple methods are based on NLP."

cat("Jaccard Similarity:", jaccard_similarity(text1, text2), "\n")
cat("Cosine Similarity:", cosine_similarity(text1, text2), "\n")
cat("Levenshtein Similarity:", 1 - normalized_levenshtein_distance(text1, text2), "\n")
cat("Winnowing Similarity:", winnowing(text1, text2), "\n")
cat("Euclidean Similarity:", euclidean_similarity(text1, text2), "\n")
cat("Overlap Coefficient:", overlap_coefficient(text1, text2), "\n")
```

## Write matrix to excel

```{r}
# Load the necessary library
library(writexl)

# Define the xlsx_writer function
xlsx_writer <- function(users, similarities) {
  # Define the file path of the output
  output_file_path <- "output/R output"

  # Get current datetime as a string (format: YYYYMMDD%H%M)
  datetime <- format(Sys.time(), "%Y%m%d%H%M")

  # Create a list to hold each sheet
  sheets <- list()

  # Create similarity sheets
  for (similarity_name in names(similarities)) {
    matrix <- similarities[[similarity_name]]

    # Check if matrix is valid (not NULL or empty)
    if (is.null(matrix) || length(matrix) == 0) {
      cat("Warning: Similarity matrix for", similarity_name, "is empty or null.\n")
      next  # Skip to the next similarity matrix
    }

    # Convert the matrix into a data frame with row and column names
    df <- data.frame(user_id = users, matrix)
    rownames(df) <- users
    colnames(df) <- c("user_id", users)
    df <- df[, c("user_id", users)]

    # Print the first 5 rows along with column names for debugging
    print(head(df, 5))

    # Add the similarity matrix to the list of sheets
    sheets[[similarity_name]] <- df
  }

  # Write all the sheets to an Excel file
  file_path <- paste0(output_file_path, "/similarity_checker(", datetime, ").xlsx")
  write_xlsx(sheets, path = file_path)

  # Print the success message
  cat("Excel file - 'similarity_checker(", datetime, ").xlsx' has been created.\n")
}

# Test
# Given 3 users
users <- c("user_1", "user_2", "user_3")

# Given different similarity matrix
similarities <- list(
  "cosine_similarity" = matrix(c(NA, 0.75, 0.10,
                                 0.75, NA, 0.50,
                                 0.10, 0.50, NA),
                               nrow = 3, byrow = TRUE),
  "euclidean_similarity" = matrix(c(NA, 0.80, 0.65,
                                    0.80, NA, 0.55,
                                    0.65, 0.55, NA),
                                  nrow = 3, byrow = TRUE)
)

# Call the function to write the Excel file
# xlsx_writer(users, similarities)
```

## Generate Similarity Matrix in Excel

```{r}
# Function to handle duplicate user_ids by adding suffixes
make_unique_ids <- function(user_ids) {
  # Convert user_ids to character to ensure proper handling
  user_ids <- as.character(user_ids)

  # Initialize occurrence tracker as an empty list
  occurrence_tracker <- list()

  # Iterate through user_ids and add suffix if necessary
  unique_ids <- sapply(user_ids, function(id) {
    # If the user_id is not in the tracker, initialize it
    if (!(id %in% names(occurrence_tracker))) {
      occurrence_tracker[[id]] <- 1  # First occurrence
      return(id)
    } else {
      occurrence_tracker[[id]] <- occurrence_tracker[[id]] + 1
      return(paste0(id, "_", occurrence_tracker[[id]]))  # Add suffix for duplicates
    }
  })

  # Check if there are still duplicates after processing
  if (any(duplicated(unique_ids))) {
    cat("Duplicate user_ids found after processing:\n", unique_ids[duplicated(unique_ids)], "\n")
    stop("Error: Duplicate user_ids remain after processing.")
  }

  return(unique_ids)
}
```

```{r}
# Load necessary libraries
library(dplyr)

# Define the compare_matrix_generator function in R
compare_matrix_generator <- function(input_file_path, filter_conditions) {
  start_time <- Sys.time()

  # Step 1: Read the CSV file and extract "user_id" and "final_submission"
  df <- csv_reader(input_file_path, filter_conditions = filter_conditions)
  user_ids <- df$user_id
  final_submissions <- df$final_submission
  cat("The size of the df:", nrow(df), "\n")

  # Step 1-2: Make user_ids unique by adding suffixes to duplicates
  unique_user_ids <- make_unique_ids(user_ids)
  # Check if unique_user_ids has duplicates after processing
  if (any(duplicated(unique_user_ids))) {
    stop("Error: Duplicate user_ids remain after processing.")
  }

  # Step 2: Prepare an empty list to store all similarity matrices
  similarities <- list(
    "cosine_similarity" = list()
    # "euclidean_similarity" = list(),
    # "jaccard_similarity" = list(),
    # "levenshtein_similarity" = list(),
    # "overlap_similarity" = list(),
    # "winnowing_similarity" = list()
  )

  # Step 3: Calculate the similarity between each text and generate the corresponding matrix
  for (i in seq_along(final_submissions)) {
    row_cosine <- numeric(length(final_submissions))
    # row_euclidean <- numeric(length(final_submissions))
    # row_jaccard <- numeric(length(final_submissions))
    # row_levenshtein <- numeric(length(final_submissions))
    # row_overlap <- numeric(length(final_submissions))
    # row_winnowing <- numeric(length(final_submissions))

    for (j in seq_along(final_submissions)) {
      if (i == j) {
        # Similarity with itself is NaN
        row_cosine[j] <- NA
        # row_euclidean[j] <- NA
        # row_jaccard[j] <- NA
        # row_levenshtein[j] <- NA
        # row_overlap[j] <- NA
        # row_winnowing[j] <- NA
      } else {
        # Calculate different similarities
        row_cosine[j] <- cosine_similarity(final_submissions[i], final_submissions[j])
        # row_euclidean[j] <- euclidean_similarity(final_submissions[i], final_submissions[j])
        # row_jaccard[j] <- jaccard_similarity(final_submissions[i], final_submissions[j])
        # row_levenshtein[j] <- 1 - normalized_levenshtein_distance(final_submissions[i], final_submissions[j])
        # row_overlap[j] <- overlap_coefficient(final_submissions[i], final_submissions[j])
        # row_winnowing[j] <- winnowing(final_submissions[i], final_submissions[j], k = 3, w = 4)
      }
    }

    similarities[["cosine_similarity"]] <- rbind(similarities[["cosine_similarity"]], row_cosine)
    # similarities[["euclidean_similarity"]] <- rbind(similarities[["euclidean_similarity"]], row_euclidean)
    # similarities[["jaccard_similarity"]] <- rbind(similarities[["jaccard_similarity"]], row_jaccard)
    # similarities[["levenshtein_similarity"]] <- rbind(similarities[["levenshtein_similarity"]], row_levenshtein)
    # similarities[["overlap_similarity"]] <- rbind(similarities[["overlap_similarity"]], row_overlap)
    # similarities[["winnowing_similarity"]] <- rbind(similarities[["winnowing_similarity"]], row_winnowing)
  }

  # Step 4: Write the similarity matrix to Excel
  xlsx_writer(unique_user_ids, similarities)

  # Calculate and round the process time
  process_time <- round(difftime(Sys.time(), start_time, units = "secs"), 2)
  cat("Process Time:", process_time, "s\n")

  return(similarities)
}

# Test the function
file_path <- 'data/text_data/extracted_behavior_pattern_data.csv'

# Define multiple filter conditions as strings
filter_conditions <- list(
  "use_ai == 1",
  "task_type == 'PRACTICAL'"
)

# Call the function with the file path and filter conditions
similarities <- compare_matrix_generator(input_file_path = file_path, filter_conditions = filter_conditions)
```